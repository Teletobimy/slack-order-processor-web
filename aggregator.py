# -*- coding: utf-8 -*-
import json
import os
from typing import Dict, List, Any, Optional
from collections import defaultdict
from excel_parser import ExcelParser
from gpt_matcher import GPTMatcher

class DataAggregator:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """ë°ì´í„° ì§‘ê³„ í´ë˜ìŠ¤ ì´ˆê¸°í™”"""
        self.excel_parser = ExcelParser()
        self.gpt_matcher = GPTMatcher(config)
        
    def process_excel_files(self, downloaded_files: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ë‹¤ìš´ë¡œë“œëœ Excel íŒŒì¼ë“¤ì„ ì²˜ë¦¬
        """
        excel_products = []
        
        for file_data in downloaded_files:
            filepath = file_data.get("filepath")
            if not filepath:
                continue
            
            print(f"Excel íŒŒì¼ ì²˜ë¦¬ ì¤‘: {filepath}")
            
            # Excel íŒŒì¼ íŒŒì‹±
            products = self.excel_parser.parse_excel_file(filepath)
            
            for product in products:
                # í’ˆëª©ì½”ë“œ ë§¤ì¹­
                match_result = self.gpt_matcher.match_product_to_code(product["product_name"])
                if match_result:
                    excel_products.append({
                        "product_name": product["product_name"],
                        "quantity": product["quantity"],
                        "í’ˆëª©ì½”ë“œ": match_result["í’ˆëª©ì½”ë“œ"],
                        "ë§¤ì¹­ëœ_ì œí’ˆëª…": match_result["ì œí’ˆëª…"],
                        "ë¸Œëœë“œ": match_result["ë¸Œëœë“œ"],
                        "confidence": match_result["confidence"],
                        "source": "excel_file",
                        "source_file": product["source_file"],
                        "row_index": product["row_index"]
                    })
        
        return excel_products
    
    def aggregate_products(self, processed_messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ëª¨ë“  ë©”ì‹œì§€ì—ì„œ ì œí’ˆ ì •ë³´ë¥¼ ì§‘ê³„ (RAG ë°©ì‹ ì‚¬ìš©)
        """
        print("ì œí’ˆ ì •ë³´ ì§‘ê³„ ì‹œì‘...")
        
        all_products = []
        ambiguous_products = []
        thread_summaries = []
        
        for i, message_data in enumerate(processed_messages):
            print(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘: {i+1}/{len(processed_messages)}")
            
            # RAG ë°©ì‹: ì „ì²´ ë©”ì‹œì§€ ë¬¸ë§¥ í™œìš©
            original_message = message_data.get("original_message", {})
            replies = message_data.get("replies", [])
            
            # ì „ì²´ í…ìŠ¤íŠ¸ ì¡°í•©
            full_text = original_message.get("text", "")
            for reply in replies:
                full_text += " " + reply.get("text", "")
            
            if full_text.strip():
                # RAG ë§¤ì¹­ ì‚¬ìš©
                rag_products = self.gpt_matcher.match_products_with_context(full_text, original_message.get("user", {}))
                
                for product in rag_products:
                    # ambiguous ë¶„ë¦¬
                    if product.get("ambiguous", False):
                        ambiguous_products.append(product)
                    else:
                        all_products.append(product)
            
            # Excel íŒŒì¼ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
            downloaded_files = message_data.get("downloaded_files", [])
            if downloaded_files:
                excel_products = self.process_excel_files(downloaded_files)
                all_products.extend(excel_products)
            
            # ìŠ¤ë ˆë“œ ìš”ì•½ ìƒì„±
            if all_products:
                thread_summaries.append({
                    "thread_index": i,
                    "summary": "ì¶œê³  ì²˜ë¦¬",
                    "product_count": len([p for p in all_products if "thread_index" not in p])
                })
        
        # ë¸Œëœë“œë³„, ì œí’ˆë³„ ìˆ˜ëŸ‰ ì§‘ê³„
        aggregated_by_brand = self.aggregate_by_brand_and_product(all_products)
        
        return {
            "aggregated_by_brand": aggregated_by_brand,
            "ambiguous_products": ambiguous_products,
            "thread_summaries": thread_summaries,
            "total_products": len(all_products),
            "unique_products": sum(len(products) for products in aggregated_by_brand.values()),
            "brands": list(aggregated_by_brand.keys())
        }
    
    def aggregate_by_brand_and_product(self, products: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """
        ë¸Œëœë“œë³„, í’ˆëª©ì½”ë“œë³„ë¡œ ì œí’ˆ ìˆ˜ëŸ‰ ì§‘ê³„
        """
        # ë¸Œëœë“œë³„ë¡œ ê·¸ë£¹í™”
        brand_groups = defaultdict(lambda: defaultdict(list))
        
        for product in products:
            brand = product.get("ë¸Œëœë“œ")
            product_code = product.get("í’ˆëª©ì½”ë“œ")
            if brand and product_code:
                brand_groups[brand][product_code].append(product)
        
        # ì§‘ê³„ëœ ê²°ê³¼ ìƒì„±
        aggregated_by_brand = {}
        
        for brand_name, brand_products in brand_groups.items():
            brand_aggregated = []
            
            for product_code, product_list in brand_products.items():
                # ìˆ˜ëŸ‰ì„ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ í•©ì‚°
                total_quantity = sum(int(p["quantity"]) if isinstance(p["quantity"], (int, str)) else 0 for p in product_list)
                
                # ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ì˜ ì œí’ˆëª… ì‚¬ìš©
                best_match = max(product_list, key=lambda x: x.get("confidence", 0) if isinstance(x.get("confidence"), (int, float)) else 0)
                
                # ì¶œì²˜ ì •ë³´ ìˆ˜ì§‘
                sources = []
                for p in product_list:
                    source_info = f"{p.get('source', 'unknown')}"
                    if p.get('source_file'):
                        source_info += f"({p['source_file']})"
                    sources.append(source_info)
                
                brand_aggregated.append({
                    "í’ˆëª©ì½”ë“œ": product_code,
                    "ì œí’ˆëª…": best_match.get("ë§¤ì¹­ëœ_ì œí’ˆëª…", ""),
                    "ì´_ìˆ˜ëŸ‰": total_quantity,
                    "ì‹ ë¢°ë„": best_match.get("confidence", 0),
                    "ì¶œì²˜_ìˆ˜": len(product_list),
                    "ì¶œì²˜_ëª©ë¡": list(set(sources)),
                    "ìƒì„¸_ì •ë³´": product_list
                })
            
            # ìˆ˜ëŸ‰ ìˆœìœ¼ë¡œ ì •ë ¬
            brand_aggregated.sort(key=lambda x: x["ì´_ìˆ˜ëŸ‰"], reverse=True)
            aggregated_by_brand[brand_name] = brand_aggregated
        
        return aggregated_by_brand
    
    def validate_aggregation(self, aggregated_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì§‘ê³„ ê²°ê³¼ ê²€ì¦ ë° í†µê³„ ìƒì„±
        """
        products = aggregated_data.get("aggregated_products", [])
        
        # í†µê³„ ê³„ì‚°
        total_quantity = sum(p["ì´_ìˆ˜ëŸ‰"] for p in products)
        avg_confidence = sum(p["ì‹ ë¢°ë„"] for p in products) / len(products) if products else 0
        
        # ì‹ ë¢°ë„ë³„ ë¶„í¬
        high_confidence = len([p for p in products if p["ì‹ ë¢°ë„"] >= 80])
        medium_confidence = len([p for p in products if 60 <= p["ì‹ ë¢°ë„"] < 80])
        low_confidence = len([p for p in products if p["ì‹ ë¢°ë„"] < 60])
        
        # ì¶œì²˜ë³„ ë¶„í¬
        source_stats = defaultdict(int)
        for p in products:
            for source in p["ì¶œì²˜_ëª©ë¡"]:
                source_stats[source] += 1
        
        validation_result = {
            "total_products": len(products),
            "total_quantity": total_quantity,
            "average_confidence": round(avg_confidence, 2),
            "confidence_distribution": {
                "high (80+)": high_confidence,
                "medium (60-79)": medium_confidence,
                "low (<60)": low_confidence
            },
            "source_distribution": dict(source_stats),
            "thread_count": len(aggregated_data.get("thread_summaries", [])),
            "validation_passed": len(products) > 0 and avg_confidence > 50
        }
        
        return validation_result
    
    def save_aggregated_data(self, aggregated_data: Dict[str, Any], filename: str = "aggregated_data.json"):
        """
        ì§‘ê³„ëœ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        """
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(aggregated_data, f, ensure_ascii=False, indent=2)
        print(f"ì§‘ê³„ ë°ì´í„° ì €ì¥: {filename}")
    
    def get_summary_report(self, aggregated_data: Dict[str, Any]) -> str:
        """
        ì§‘ê³„ ê²°ê³¼ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        """
        products = aggregated_data.get("aggregated_products", [])
        validation = self.validate_aggregation(aggregated_data)
        
        report = f"""
=== ì œí’ˆ ì§‘ê³„ ê²°ê³¼ ë¦¬í¬íŠ¸ ===

ğŸ“Š ê¸°ë³¸ í†µê³„
- ì´ ì œí’ˆ ì¢…ë¥˜: {validation['total_products']}ê°œ
- ì´ ìˆ˜ëŸ‰: {validation['total_quantity']}ê°œ
- í‰ê·  ì‹ ë¢°ë„: {validation['average_confidence']}%
- ì²˜ë¦¬ëœ ìŠ¤ë ˆë“œ: {validation['thread_count']}ê°œ

ğŸ¯ ì‹ ë¢°ë„ ë¶„í¬
- ë†’ìŒ (80%+): {validation['confidence_distribution']['high (80+)']}ê°œ
- ë³´í†µ (60-79%): {validation['confidence_distribution']['medium (60-79)']}ê°œ
- ë‚®ìŒ (60% ë¯¸ë§Œ): {validation['confidence_distribution']['low (<60)']}ê°œ

ğŸ“‹ ìƒìœ„ ì œí’ˆ (ìˆ˜ëŸ‰ ê¸°ì¤€)
"""
        
        for i, product in enumerate(products[:10], 1):
            report += f"{i}. {product['ì œí’ˆëª…']} (ì½”ë“œ: {product['í’ˆëª©ì½”ë“œ']}) - {product['ì´_ìˆ˜ëŸ‰']}ê°œ\n"
        
        if validation['validation_passed']:
            report += "\nâœ… ê²€ì¦ í†µê³¼: ë°ì´í„° í’ˆì§ˆì´ ì–‘í˜¸í•©ë‹ˆë‹¤."
        else:
            report += "\nâš ï¸ ê²€ì¦ ì‹¤íŒ¨: ë°ì´í„° í’ˆì§ˆì„ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        return report

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    aggregator = DataAggregator()
    
    # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ê°€ ìˆë‹¤ë©´
    test_file = "processed_slack_data.json"
    if os.path.exists(test_file):
        with open(test_file, 'r', encoding='utf-8') as f:
            processed_messages = json.load(f)
        
        aggregated_data = aggregator.aggregate_products(processed_messages)
        print(aggregator.get_summary_report(aggregated_data))
        
        aggregator.save_aggregated_data(aggregated_data)
    else:
        print("í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
